\section{Estimating a sum}

In order to estimate sums of the form $\sum_{n=1}^N \frac{b_n^t}{n^\sigma}$ for very large values of $N$, the following estimate will be used.

\begin{lemma}\label{largen}
Let $N \geq N_0 \geq 1$ be natural numbers, and let $\sigma,t > 0$ be such that
$$ \sigma > \frac{t}{2} \log N.$$
Then
$$ \sum_{n=1}^N \frac{b_n^t}{n^\sigma} \leq \sum_{n=1}^{N_0}
\frac{b_n^t}{n^\sigma}  + 
\max( N_0^{1-\sigma} b_{N_0}^t, N^{1-\sigma} b_N^t ) \log \frac{N}{N_0}.$$
\end{lemma}

\begin{proof}  From the identity
$$ \frac{b_n^t}{n^\sigma} = \frac{\exp\left( \frac{t}{4} (\log N - \log n)^2 - \frac{t}{4} (\log N)^2\right) }{n^{\sigma - \frac{t}{2} \log N}}$$
we see that the summands $\frac{b_n^t}{n^\sigma}$ are decreasing for $1 \leq n \leq N$, hence by the integral test one has
\begin{equation}\label{rado}
 \sum_{n=1}^N \frac{b_n^t}{n^\sigma} \leq \sum_{n=1}^{N_0}
\frac{b_n^t}{n^\sigma}  + \int_{N_0}^N \frac{b_a^t}{a^\sigma}\ da.
\end{equation}
Making the change of variables $a = e^u$, the right-hand side becomes
$$\sum_{n=1}^{N_0} \frac{b_n^t}{n^\sigma} \exp( (1-\sigma) u + \frac{t}{4} u^2 )\ du.$$
The expression $(1-\sigma) u + \frac{t}{4} u^2$ is convex in $u$, and is thus bounded by the maximum of its values at the endpoints $u = \log N_0, \log N$; thus
$$\exp( (1-\sigma) u + \frac{t}{4} u^2) \leq N_0^{1-\sigma} b_{N_0}^t, N^{1-\sigma} b_N^t.$$
The claim follows. 
\end{proof}

\begin{remark}  The right-hand side of \eqref{rado} can be evaluated exactly as
$$
\sum_{n=1}^{N_0}
\frac{b_n^t}{n^\sigma}  + \frac{\sqrt \pi}{\sqrt t} \exp(\frac{-(\sigma - 1)^2}{t}) \left( \operatorname{erfi}\left(\frac{\frac{t}{2} \log N  - \sigma + 1}{\sqrt t} \right) - \operatorname{erfi}\left(\frac{\frac{t}{2} \log N_0  - \sigma + 1}{\sqrt t}\right) \right).$$

In practice, this upper bound for $\sum_{n=1}^N \frac{b_n^t}{n^\sigma}$ is slightly more accurate than the one in Lemma \ref{largen}, and is a good approximation even for relatively small values of $N_0$ (e.g., $N_0=100$).  However, the cruder bound above suffices for the numerical values of parameters needed to establish the bound $\Lambda \leq 0.22$.
\end{remark}

\subsection{Fast evaluation of multiple values of $f_t(s)$}\label{Fast}

Fix $t \geq 0$.  For the verification of the barrier criterion (Theorem \ref{ubc-0}(iii)) using Corollary \ref{zero-test}, we will need to evaluate the quantity $f_t(s)$ to reasonable accuracy for a large number of values of $s$ in the vicinity of a fixed complex number $X+iy$.  From \eqref{ft-def} we have
\begin{equation}\label{fts}
f_t(s) = \sum_{n=1}^N \frac{n^b b_n^t}{n^{\frac{1+y-iX}{2}}} + \gamma(s) \sum_{n=1}^N \frac{n^a b_n^t}{n^{\frac{1-y+iX}{2}}},
\end{equation}
where $b_n^t$ is given by \eqref{bn-def}, $\gamma(s)$ is given by \eqref{lambda-def}, $N$ is given by \eqref{N-def-main} and
$$ b = b(s) \coloneqq  \frac{1+y-iX}{2} - s_* $$
and
$$ a = a(s) \coloneqq  \frac{1-y-iX}{2} - \overline{s_*} - \kappa$$
with $s_*, \kappa$ defined by \eqref{sn-def}, \eqref{kappa-def}.  In practice the exponents $a,b$ will be rather small, and $N$ will be fixed (in our main verification we will in fact have $N = 69098$).

A naive computation of $f_t(s)$ for $M$ values of $s$ would take time $O(NM)$, which turns out to be somewhat impractical for for the ranges of $N,M$ we will need; indeed, for our main theorem, the total number of pairs $(t,s)$ at which we need to perform the evaluation is $785052$ (spread out over $152$ values of $t$), and direct computation of all this data required $78.5$ hours of computer time, which was still feasible at this order of magnitude of $X$ but would not scale to significantly higher magnitudes.  However, one can significantly speed up the computation (to about $0.025$ hours) to extremely high accuracy by using Taylor series expansion to factorise the sums in \eqref{fts} into combinations of sums that do not depend on $s$ and thus can be computed in advance.

We turn to the details.  To make the Taylor series converge\footnote{One can obtain even faster speedups here by splitting the summation range $\sum_{n=1}^N$ into shorter intervals and using a Taylor expansion for each interval, although ultimately we did not need to exploit this.} faster, we recenter the sum in $n$, writing
$$ \sum_{n=1}^N F(n) = \sum_{h=-\lfloor N/2\rfloor+1}^{\lfloor (N+1)/2\rfloor} F(n_0 + h)$$
for any function $F$, where $n_0 \coloneqq \lfloor N/2$.  We thus have
$$ f_t(s) = B(b) + \gamma(s) A(n_0,a)$$
where
$$ B(b) \coloneqq \sum_{h=-\lfloor N/2\rfloor+1}^{\lfloor (N+1)/2\rfloor} \frac{(n_0+h)^b b_{n_0+h}^t}{(n_0+h)^{\frac{1+y-iX}{2}}}$$
and
$$ A(a) \coloneqq \sum_{h=-\lfloor N/2\rfloor+1}^{\lfloor (N+1)/2\rfloor} \frac{(n_0+h)^a b_{n_0+h}^t}{(n_0+h)^{\frac{1-y+iX}{2}}}.$$
We discuss the fast computation of $B(b)$ for multiple values of $b$; the discussion for $A(a)$ is analogous.  We can write the numerator $(n_0+h)^b b_{n_0+h}^t$ as
$$ \exp( b \log(n_0+h) + \frac{t}{4} \log^2(n_0+h) );$$
writing $\log(n_0+h) = \log n_0 + \log(1+\frac{h}{n_0})$, this becomes
$$ n_0^{b + \frac{t}{4} \log n_0} \exp( \frac{t}{4} \log^2(1+\frac{h}{n_0}) ) \exp( (b + \frac{t}{2} \log n_0) \log(1+\frac{h}{n_0}) ).$$
By Taylor expanding\footnote{It is also possible to proceed by just performing Taylor expansion on the second exponential and leaving the first exponential untouched; this turns out to lead to a comparable numerical run time.} the exponentials, we can write this as
$$ n_0^{b + \frac{t}{4} \log n_0} \sum_{i=0}^\infty \sum_{j=0}^\infty \frac{( \frac{t}{4} \log^2(1+\frac{h}{n_0}) )^i}{i!} \log^j(1+\frac{h}{n_0}) \frac{(b+\frac{t}{2} \log n_0)^j}{j!}$$
and thus the expression $B(b)$ can be written as
$$ B(b) = n_0^{b + \frac{t}{4} \log n_0} \sum_{i=0}^\infty \sum_{j=0}^\infty B_{i,j} \frac{(b+\frac{t}{2} \log n_0)^j}{j!}$$
where
$$ B_{i,j} \coloneqq \sum_{h=-\lfloor N/2\rfloor+1}^{\lfloor (N+1)/2\rfloor} \frac{( \frac{t}{4} \log^2(1+\frac{h}{n_0}) )^i}{i!} \frac{\log^j(1+\frac{h}{n_0})}{(n_0+h)^{\frac{1+y-iX}{2}}}.$$
If we truncate the $i,j$ summations at some cutoff $E$, we obtain the approximation
$$ B(b) \approx n_0^{b + \frac{t}{4} \log n_0} \sum_{i=0}^{E-1} \sum_{j=0}^{E-1} B_{i,j}(n_0) \frac{(b+\frac{t}{2} \log n_0)^i}{i!}.$$
The quantities $B_{i,j}, i,j=0,\dots,{E-1}$ may be evaluated in time $O(N E^2)$, and then the sums $B(b)$ for $M$ values of $b$ may be evaluated in time $O(ME^2)$, leading to a total computation time of $O((N+M) E^2)$ which can be significantly faster than $O(NM)$ even for relatively large values of $E$.  We took $E=50$, which is more than adequate to obtain extremely high accuracy for $f_t(s)$; see Figure \ref{fig1}.  The code for implementing this may be found in the file

\centerline{\tt dbn\_upper\_bound/pari/barrier\_multieval\_t\_agnostic.txt}

in the github repository \cite{github}.

